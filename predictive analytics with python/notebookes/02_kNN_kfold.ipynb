{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross-Validation with the Iris Dataset\n",
    "\n",
    "In this notebook, we'll explore **K-fold cross-validation** and apply it to the classic **Iris dataset** using `scikit-learn`. K-fold cross-validation is an essential technique for evaluating the performance of a machine learning model by splitting the data into training and testing sets multiple times.\n",
    "\n",
    "---\n",
    "\n",
    "## What is K-Fold Cross-Validation?\n",
    "\n",
    "K-fold cross-validation is a method used to:\n",
    "- Assess the generalization performance of a model.\n",
    "- Reduce the risk of overfitting by averaging the model’s performance across multiple splits.\n",
    "  \n",
    "In K-fold cross-validation, the data is divided into `K` subsets (or \"folds\"). The model is trained on `K-1` folds and evaluated on the remaining fold, repeating this process `K` times so that each fold serves as the test set exactly once. The final evaluation score is the average performance over all `K` folds.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Import Libraries\n",
    "\n",
    "Let's start by importing the necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the Iris Dataset\n",
    "\n",
    "We'll use the Iris dataset, a popular dataset in machine learning. It consists of 150 samples of iris flowers, each described by four features: `sepal length`, `sepal width`, `petal length`, and `petal width`. The target variable represents three species of iris: *Setosa*, *Versicolour*, and *Virginica*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X = data.data  # Features\n",
    "y = data.target  # Target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set Up K-Fold Cross-Validation\n",
    "\n",
    "We'll use `KFold` from `scikit-learn` to split our data into `K=5` folds. This means the data will be split into 5 subsets, and the model will be trained on 4 folds and evaluated on the remaining 1 fold, repeating this process 5 times.\n",
    "\n",
    "- `n_splits=5` specifies the number of folds.\n",
    "- `shuffle=True` shuffles the data before splitting to ensure each fold is representative of the overall dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of folds\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of K-Fold Splits\n",
    "\n",
    "In **K-fold cross-validation**, each fold serves as a test set exactly once, while the remaining folds serve as the training set. \n",
    "\n",
    "Since we have **150 samples** in the Iris dataset (50 samples for each of the three species) and we set `k=5` folds, here’s how the data is divided:\n",
    "\n",
    "- **Total samples in each fold**: With 5 folds, each fold will have $ \\frac{150}{5} = 30 $ samples as the test set.\n",
    "- **Training samples**: The remaining samples (150 - 30 = 120) will be used for training in each fold.\n",
    "\n",
    "For each fold:\n",
    "- **Training set**: 120 samples (approximately 40 from each species).\n",
    "- **Test set**: 30 samples (approximately 10 from each species).\n",
    "\n",
    "Here’s a table to illustrate how data might be split across 5 folds:\n",
    "\n",
    "| Fold Number | Training Data Indexes (120 Samples) | Test Data Indexes (30 Samples) |\n",
    "|-------------|-------------------------------------|--------------------------------|\n",
    "| Fold 1      | Example: 1, 2, 3, ..., 150          | Example: 21, 44, ..., 130      |\n",
    "| Fold 2      | Example: 1, 3, 4, ..., 149          | Example: 2, 5, ..., 78         |\n",
    "| Fold 3      | Example: 1, 2, 5, ..., 148          | Example: 3, 9, ..., 143        |\n",
    "| Fold 4      | Example: 1, 2, 4, ..., 147          | Example: 10, 12, ..., 135      |\n",
    "| Fold 5      | Example: 2, 4, 5, ..., 150          | Example: 1, 7, ..., 119        |\n",
    "\n",
    "This structure ensures each sample is used for both training and testing, providing a more comprehensive evaluation of the model’s performance.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train and Evaluate the Model with K-Fold Cross-Validation\n",
    "\n",
    "We'll use the **K-Nearest Neighbors (KNN)** algorithm with `k=3` neighbors to classify the iris flowers. For each fold, we'll:\n",
    "1. Split the data into training and test sets.\n",
    "2. Train the model on the training set.\n",
    "3. Predict on the test set.\n",
    "4. Calculate and store the accuracy score.\n",
    "\n",
    "Finally, we'll calculate the **average accuracy** across all folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies for each fold: [1.0, 0.9666666666666667, 0.9666666666666667, 0.9333333333333333, 0.9666666666666667]\n",
      "Average Accuracy across all folds: 0.97\n"
     ]
    }
   ],
   "source": [
    "\n",
    "k_neighbors = 3\n",
    "accuracies = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=k_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"Accuracies for each fold: {accuracies}\")\n",
    "print(f\"Average Accuracy across all folds: {average_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Results\n",
    "\n",
    "The output shows:\n",
    "- The **accuracy for each fold** individually.\n",
    "- The **average accuracy** across all folds, giving an overall performance measure for the model.\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages of K-Fold Cross-Validation\n",
    "\n",
    "1. **Reduced Bias**: K-fold cross-validation provides a better generalization estimate by training and testing on multiple subsets of data.\n",
    "2. **Efficient Use of Data**: All data points are used for both training and testing, maximizing data utilization.\n",
    "3. **Versatility**: Works well for both small and large datasets.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
